\section{\smrsystem Background} \label{sec:background}

\smrsystem's deployment model is similar to a typical \smr's. In a \smrsystem-replicated 
system, a set of $2f+1$ machines (nodes) are set up in a InfiniBand cluster. Once the 
\smrsystem system starts, one node becomes the \emph{primary} node which proposes the order of 
requests to execute, and the others become backup nodes which follow the primaryâ€™s 
proposals. An arbitrary number of clients in LAN or WAN send network requests to the 
primary and get responses. If failures occur, the nodes run a leader election to elect 
a new leader and continue.

On receiving a client network request, it invokes a RDMA-based consensus process on this 
request to enforce that all replicas see the same sequence of input requests. This process 
has three steps. In the first step, the leader assigns a global, monotonically increasing 
viewstamp to this request, stores this request into an entry that is appended to the consensus 
log, and does a forced write to the local disk. The second step is to replicate the log entry 
on remote servers using a one-sided RDMA Write operation. Usually when the RDMA NIC (RNIC) 
completes the network steps associated with the RDMA operation, it pushes a completion event 
to the queue pair's associated completion queue (CQ) via a DMA write. Using completion events 
adds extra overhead. Since \paxos could help handle the reliability issues, \smrsystem takes 
advantage of unsignaled RDMA write operations, \ie, a completion event will not be pushed for 
these operations, to reduce that overhead. In the last step, the leader thread waits for 
acknowledgments from a majority of nodes.

In addition to the distributed consensus protocol for coordinating the sequence of input requests, 
\smrsystem also runs an output checking protocol which compares each replica's network outputs 
occasionally to detect the divergence of execution states.
