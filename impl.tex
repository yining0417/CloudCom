% \section{Implementation} \label{sec:impl}

% This section first introduces the implementation details of \xxx's \paxos 
% component, and then presents the implementation choices of the checkpoint 
% component.

% \subsection{\paxos Protocol} \label{sec:paxos}
% % SMR. Paxos. Choose it because: (1) fast. (2) transparent to applications.
% The \paxos consensus component is a critical component to enforce a 
% consistent 
% total order of socket operations from client programs. Although there are 
% already a number of open source \paxos implementations~\cite{concoord, 
% zookeeper, chubby:osdi, libpaxos}, we find it necessary to re-implement a 
% \paxos protocol because it is hard to port their current consensus 
% interface to \xxx's socket consensus interface.
% 
% 
% We have implemented a standard \paxos protocol based on a well-known approach 
% called ``\paxos Made Practical"~\cite{paxos:practical} because it is easy to 
% understand, use, and fast. In normal case, only the primary node is proposing 
% consensus requests, so consensus can be achieved fast. When exceptional cases 
% such as network partitions and node failures occur, a \paxos leader election 
% is 
% invoked to resolve conflicts. Each socket operation from the client is 
% assigned 
% a global, monotonically increasing viewstamp (or \emph{global index}) to 
% identify each checkpoint. Upon consensus on a socket operation, each 
% consensus 
% component persistently stores the operation type, arguments, and global index 
% into Berkeley DB storage on local SSD.



\subsection{Checkpoint and Rollback Mechanism} \label{sec:checkpoint}

To support synchronous analysis tools (\eg, control flow integrity and buffer 
overrun protection tools) in \xxx, we have designed two checkpoint and rollback 
functions. When an analysis tool calls the \v{checkpoint()} function, \xxx uses 
its \paxos consensus component to invoke an operation: ``take a checkpoint 
associated with the global index of the latest socket operation". Once a 
consensus is reached, all replicas (including primary) do a checkpoint 
operation as the next consensus operation.


When the tool catches a malicious event and decides to roll back to a previous
checkpoint associated with a socket operation index, the tool calls a 
\v{rollback(int} \v{index=-1,} \v{bool discard=true)} function. Ignoring the 
arguments in this function means rolling back to the last checkpoint within all 
replicas and discard all inputs since this checkpoint. This API works as three 
steps: (1) current replica uses the \paxos consensus component to request an 
operation: ``rollback to a previous checkpoint according to \v{index}"; (2) 
once consensus is reached, each replica invokes \criu to do the actual rollback 
operation; (3) if \v{discard} is on, each replica discards all future inputs 
since the checkpoint. This API makes all replicas rollback and discard 
malicious inputs consistently.


\xxx's checkpoint mechanism has a few limitations: (1) although this mechanism's 
API is expressive, they moderately trade off transparency between an analysis 
tool and \xxx's framework; (2) it may defer the processing of network requests; 
and (3) it can not revoke information that have already leaked to clients. 
However, considering the benefits (\S\ref{sec:strengthen-analysis}) that \xxx 
may bring to tools, we argue that this checkpoint mechanism is still worthwhile.

% Second, once an anlysis 
% captures some bad events caued by processing malicious inputs, \xxx must 
% allow % the analysis to invoke an operation to consistently roll back all 
% nodes's % execution states before this malicious input is being processed, 
% and % allow the % analysis to determine re-executing the inputs or discarding 
% the % inputs.

% To transparently checkpoint an application, \xxx's checkpointer component 
% leverages a popular, open source checkpoint-restore tool called 
% \criu~\cite{criu}. This tool has supported CPU registers, memory, etc. 
% However, 
% one key issue still exists: a server constantly serves requests, and 
% checkpointing and restoring TCP stacks on different machines is notoriously 
% difficult. Our trick to avoid this difficulty is that we have observed that 
% server programs always have some idle moments. Thus, a \xxx non-primary 
% replica 
% running an actual execution does a checkpoint periodically (one minute by 
% default) when the replica has no alive socket connections.

% For the checkpoint frequency on analysis nodes, we design it to be high, 
% because: (1) analyses may need to roll back whenever detecting an malicious 
% events; and (2) the time cost by a checkpoint operation does not matter much 
% because the analysis is heavyweight anyway. Thus, a \xxx replica running an 
% analysis performs a checkpoint at the moment the server has finished serving 
% the current batch of socket connections: once the \paxos consensus component 
% on 
% each node finds that currently no socket connections are alive, the replica 
% does a checkpoint. This checkpoint moment is also good for \xxx's rollback 
% API 
% because it helps the analysis get rid of the current socket connections which 
% trigger the malicious events. 


% To meet the second requirement, we create an API that can be called by an 
% analysis tool to invoke a rollback. The API format is: \v{int} 
% \v{rollback(int} % \v{ncheckpoints = 0,} \v{bool skip = false)}. Once the 
% analysis calls this % operation, it invokes a client to send an roll back 
% operation to the primary node with the the list of the last checkpoint's 
% global % indexes. The primary then invokes a consensus on an internal roll 
% back % opration, % with the 





% To support synchronous analysis tools, \xxx's checkpointer component can be 
% invoked 
% by an analysis tool and consistently roll back to safe 
% execution states. Each checkpoint is associated with the index of the last 
% executed 
% socket operation, so that \xxx can consistently match up with the execution 
% states of native 
% executions and various analysis executions. To perform checkpoints 
% transparently without affecting application's executions and analyses, \xxx 
% leverages \criu~\cite{criu}, a popular, open source process checkpoint tool 
% that supports 
% CPU registers, memory, etc. Each checkpoint operation is only performed on the 
% server program and the \dmt scheduler; the \paxos consensus component does not 
% require checkpoints because we explicitly design it to be stateless (all socket 
% operations have been persistently logged). 

% When a synchronous analysis tool determines that there may be a potential 
% malicious event ahead, 
% it calls a \v{checkpoint()} function, which uses \xxx's own \paxos component to 
% invoke a consensus on checkpointing each replica as the next operation. When 
% the tool 
% catches a malicious event and decides to roll back to a 
% checkpoint associated with a socket operation index, the analysis can call a 
% \v{rollback(int} \v{index=-1,} \v{bool discard=true)} API provided 
% by \xxx. Ignoring the argument in this API means rolling back to the last 
% checkpoint within all replicas and discard all inputs since this checkpoint. To 
% determine a specific index to roll back, the analysis can also intercept socket 
% operations. More details on the work flow of the checkpoint and rollback APIs 
% are described in \S\ref{sec:checkpoint}.