\section{Implementation} \label{sec:impl}

\subsection{\paxos protocol} \label{sec:paxos}
% SMR. Paxos. Choose it because: (1) fast. (2) transparent to applications.
The \paxos consensus component is a critical component to enforce a consistent 
total order of socket operations from client programs. Although there are 
already a number of open source \paxos implementations~\cite{concoord, 
zookeeper, chubby:osdi, libpaxos}, we find it necessary to re-implement a 
\paxos protocol because \xxx we found it hard to port their current consensus 
interface to our socket consensus interface.

We have implemented a standard \paxos protocol based on a well-known approach 
called ``\paxos Made Practical"~\cite{paxos:practical} because it is easy to 
understand, use, and fast. In normal case, only the primary node is proposing 
consensus requests, so consensus can be achieved fast. When exceptional cases 
such as network partitions and node failures occur, a \paxos leader election is 
invoked to resolve conflicts. Each socket operation from the client is assigned 
a global, monotonically increasing viewstamp (or \emph{global index}) to 
identify each checkpoint. Upon consensus on a socket operation, each consensus 
component persistently stores the operation type, arguments, and global index 
into Berkeley DB storage on local SSD.

\subsection{Enforcing Consistent Logical Times for Socket Operations} 
\label{sec:timing}
% DMT.
% Timing. Go light.

\subsection{Checkpoint and Restore} \label{sec:checkpoint}

The goal of transparently supporting analyses requires that the checkpointer 
must be transparent to the analysis as well as the application. This not only 
means that a checkpoint operation must be done without modifying the analysis 
or the application, but also means that the checkpointer must support nodes' 
role changes (\eg, switching a node between a normal execution with an analysis 
execution, or between different analysis executions).

% Second, once an anlysis 
% captures some bad events caued by processing malicious inputs, \xxx must 
% allow % the analysis to invoke an operation to consistently roll back all 
% nodes's % execution states before this malicious input is being processed, 
% and % allow the % analysis to determine re-executing the inputs or discarding 
% the % inputs.

To meet the first requirement, \xxx's checkpointer component leverages a 
popular, open source checkpoint-restore tool called \criu~\cite{criu}. This 
tool has supported CPU registers, memory, etc. However, one key issue still 
exists: a server constantly server requests, and checkpointing and restoring 
TCP stacks on different machines is notorioursly difficult. Our trick to avoid 
this difficulty is that we have found server pro grams always have some idle 
moments. For instance, consider \apache, even running with its standard 
performance-stress benchmark \ab, we have observed that in some moments the 
server has no alive socket connection. Thus, we performs each checkpoint at the 
moment the server has finished serving the last batch of requests: once the 
\paxos consensus component on each node finds that currently no socket 
connections are alive, and a period has elapsed (10 seconds by default), we do 
a checkpoint. This mechanism also makes it each to match up the checkpoints' 
global index from machines with different roles.

% To meet the second requirement, we create an API that can be called by an 
% analysis tool to invoke a rollback. The API format is: \v{int} 
% \v{rollback(int} % \v{ncheckpoints = 0,} \v{bool skip = false)}. Once the 
% analysis calls this % operation, it invokes a client to send an roll back 
% operation to the primary node with the the list of the last checkpoint's 
% global % indexes. The primary then invokes a consensus on an internal roll 
% back % opration, % with the 
