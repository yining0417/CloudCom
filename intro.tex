\section{Introduction} \label{sec:intro}



% P1: dynamic program analysis framework is good. Traditional ones are 
% fully-coupled, but slow.
Dynamic program analysis frameworks have become increasingly pervasive and 
critical because they enable a wide range of powerful analysis tools at 
runtime. Existing analysis frameworks can be classsified into two approaches 
depending on how a framework exposes an application's actual execution states 
to an anlysis tool. Traditional analysis frameworks~\cite{dynamorio, pin:pldi05, 
valgrind:pldi, lift:micro06, tsan} take a ``fully-coupled" approach: a framework inlines an 
analysis with the execution, then the analysis can inspect all or most execution 
states. However, this approach can easily let the analysis slow down the actual 
execution. To improve performance, leveraging a fact that many analyses can be 
done asynchronously with the actual execution, some recent 
frameworks~\cite{decouple:usenix08, speck:asplos08, shadowreplica:ccs13, 
wester:parallelizing:asplos13, superpin, jungwoo:oopsla09} take a second ``partially-decoupled" approach: 
only the execution states critical to the analysis (\eg, effective memory 
addresses and thread interleavings) are exposed to the analysis, then the actual 
execution can be less perturbed by the analysis. Existing analysis frameworks 
have embraced numerous analysis tools and have greatly improved software quality 
(\eg, detecting data races in multithreaded programs).

% The second approach 
% is ``partially-decoupled" approach: to  improve performance, only a subset of 
 % frameworks because powerfuly analysis tend to be heavyweight and 
% prohibitively % slow down the execution.


Unfortunately, despite these great effort, existing analysis frameworks still 
have three problems. First, existing frameworks are too slow to deploy in 
production runs. Traditional frameworks fully couples the analysis with the 
actual execution, so whenever an analysis does some heavyweight work, the 
execution is slowed down (\eg, 20X+ slow down for many programs in a race 
detection tool~\cite{tsan}). Recent frameworks that take the 
``partially-decoupled" approach have shown to run 4X 
faster~\cite{shadowreplica, pararace} than the traditional frameworks with the 
same analysis because they transfer fewer execution states. However, the 
overall slowdown of these frameworks are still prohibitive because the amount 
of execution states (\eg, effective memory addresses and thread interleavings) 
transfered to analysis tools are still enormous.


The second problem is that the frameworks that take the ``partially-decoupled" 
approach heavily trade off transparency between the framework and analysis 
tools. To be able to transfer fewer execution states to the analysis tool, 
these frameworks heavily carve the analysis tools as well as the transfered 
execution states. For example, a recent framework for race detection tools 
leverages the record-replay technique~\cite{scribe:sigmetrics2010, 
respec:asplos10, racepro:sosp11} to off load the analysis from the actual 
execution, but this framework requires significantly carving race detection 
tools into several phases to adapt to record-replay, which makes the tools 
largely different from typical ones (Note that an analysis tool itself is 
usually extremely difficult to implement and maintain.)


The third problem is that existing frameworks have not shown to support 
multiple types of analysis tools at the same time. An application in 
production runs often demand the powerful guarantees of multiple analyses. 
However, traditional ``fully-coupled" frameworks typically take the shadow 
memory approach to hold analysis results for each memory byte in the actual 
execution, then different analyses may require different copies of 
shadow memory. It is not trivial to extend these frameworks to support multiple 
copies of shadow memory. It is not trival to extend ``paritall-decoupled" 
frameworks to support multiple types of analysis tools at the same time either, 
because these frameworks heavily carve the transfered execution states for each 
type of analysis.


In sum, a key common reason for these three problems in existing analysis 
frameworks is that they have only one actual execution, so they have to either 
fully couples the analysis with the actual execution, or partially decouples 
the analysis from the execution and then explicitly transfer a sub portion of 
execution states to the analysis.




% Existing in-line 
% analysis frameworks also only supports one analysis at the same time because 
% a % typical such frameworks typically maintain analysis results with shadow 
% memory, 
% which can only support one analysis in one execution.


% We argue that many analyses can be fully decoupled from the executions if 
% we could have such a framework that can meet three requirements. First (R1), 
% the framework must consistently replicate the same inputs for the executions 
% with and without analysis even for server applications (\eg, \apache) that 
% continuously accept inputs. Second (R2), given the same input, the framework 
% must efficiently enforce the same thread interleavings (or \emph{schedules}) 
% for execution and analysis without the need of frequently transfering 
% schedules % between them. Third (R3), when bad events occur in the execution 
% or % the % analysis (\eg, a NULL pinter dereference) caused by some malicious 
% inputs, % the % framework must provide a transparent checkpoint and restore 
% mechanism to % recover both the execution and the analysis back to an 
% equavalent % state % before % the malicious inputs are processed, and then 
% replicas % consistently % discard the % malicious inputs.


% P4: Our approach. Replication. DMT. And why we are better than existing 
% replication systems: avoid shipping schedules, avoid annotations.
To address the three problems, this paper presents \xxx, an efficient, 
transparent dynamic program analysis framework. Our key insight is that one can 
actually construct multiple equivalent executions with a technique called 
\emph{state machine replication (or \smr)}~\cite{paxos:simple, 
paxos:practical}. \smr assumes a deterministic application, runs the 
application on multiple machines (or replicas), and enforces a consistent 
sequence of inputs for the application across replicas as long as a majority of 
replicas reach consensus on each input.


However, \smr alone is not sufficient to construct multiple equivalent 
executions because today's typical applications largely use threads and are 
nondeterministic. The thread interleavings (or emph{schedules}) in different 
replica can easily lead to divergent execution states and break \smr. To 
address this problem, \xxx leverages \emph{deterministic multithreading (or 
\dmt)}, a recent advanced threading technique that can efficiently enforce that 
inter-thread communications happen in the same total order, then different 
replicas in \smr run into the same schedules and progress with the same 
execution states. In sum, by leveraging \smr and \dmt, one can construct 
equivalent execution in replicas, then \xxx can run multiple anlyses in 
minor replicas transparently, while the majority of the replicas can still 
reach consensus and process inputs efficiently.


A practical challenge for \xxx is that even some asynchronous analyses require 
rolling back to previous good execution states when they detect malicious 
events (\eg, a memory checker detects a NULL pointer dereference). To address 
this challenge, \xxx has incorporated a transparent application checkpoint and 
restore mechanism for the analysis to decide which checkpoint to roll back.


We have implemented a \xxx system propotype in Linux. Evaluation on a widely 
used server program \clamav with three replicas shows that \xxx runs 
efficiently with two Valgrind analyses (one lightweight, one heavyweight) with 
a overhead \overhead over the native un-replicated execution on 24-core 
machines.


% P5: A practical challenge. Transparent checkpoint.
% However, to make \xxx's asynchronous analysis framework practical, two 
% challenges should be addressed. First, depending on machine restarts and 
% network partitions, a node's role may change, not only between a \paxos 
% primary 
% and backup, but also between normal nodes and analysis nodes, ande even 
% between 
% different analysis nodes. We design a checkpoint mechanism that matches up 
% checkpoints from different role machines with input total order ID to address 
% this challenge. Second, we need a mechanism to roll back all replicas when a 
% bad event (\eg, deferencing a NULL pointer) has occured and detected by the 
% analysis tool. We design a a simple API for analysis tool to invoke so that 
% the 
% replicas can reach consensus on this roll back and act consistently.





% P6: Benefits. This paragraph can be removed.
The main contribution of \xxx is the idea of constructing multiple equavalent 
executions to fully decoupe execution and analysis. Leveraging the replication 
architecture, majority of replicas running the actual executions can process 
inputs fast. In addition, multiple analysis tools can run transparently within 
minor replicas, so that tool developers can now focus on the analysis logic 
itself, and can even strenghten the guarantees of analysis tools via removing 
the performance considerations.

% In the reamaining of this paper, \ref{sec:background} introduces some 
% background on \smr and \dmt, \ref{sec:overview} gives an overview of \xxx's 
% framework architecture and discusses on what types of analyses are suitable 
% for % \xxx,



% which introduces three major benefits. First, \xxx's replication 
% architecture provides an efficient framework for many asynchronous 
% analysis tools, ranging from reliability tools, to profiling tools, and to 
% logging tools. Second, unlike many previous work which tightly orchest the 
% framework with the analysis for better performance (\eg, ShadowReplica and 
% ASPLOS13), our framework is transparent to analysis logic, so it is 
% complementary to existing analyses as well as frameworks and can easily 
% deploy existing frameworks in our replicas (\eg, Valgrind). Third, the 
% flexibility on the number of replicas enable multiple analyses (up to \v{f}) 
% in % one execution. 






% Powerfuly analyses are usually heavyweight and prohibitively slow down the 
% actual execution that runs synchronously. This problem has restricted most of 
% existing frameworks as well as the analysis tools to deploy only in testing 
% labs without being able to exercise real-world workloads. In fact, many 
% analyses can be done asynchronously (\eg, data race detection and profling 
% tools), so recently researchers have proposed asynchronous frameworks (\eg, 
% Aftersight, Spec, ASPLOS13) via an advanced technique called 
% ``record-replay": % the record phase runs the actual execution and records 
% nondeterministic sources % such as inputs, while the replay phase runs 
% (typically offline) the actual % execution with an analysis tool on the 
% recorded inputs. This asynchronous % approach decouples the analysis from the 
% execution and lets the actual % execution run efficiently.


% P3: Some recent work tried to avoid the synchronizing execution states with
% an approach called record-replay. But thread interleabings still need to sync.
% and not transparent as well.
% Unfortunately, the emerging multithreading trend, which is driven by the rise 
% of the multi-core hardware, poses a significant challenge on existing 
% asynchronous frameworks. For instance, today's popular server applications 
% (\eg, \apache) use multiple threads to server requests parallelly. These 
% parallel and continuously running applications require record-replay to run 
% online (cite Respec) which frequently synchronize thread interleavings (or 
% \emph{schedules}), otherwise the record and replay may run into different 
% schedules and easily leading to divergent execution states. Despite a variety 
% of notable approaches are proposed (cite Respec, DoublePlay, and ASPLOS 
% 2013), 
% efficient and scalable online record-replay is still an open problem. For 
% instance, two notable systems incur 28\% to 55\% overhead with four threads 
% in 
% record phase.

% For 
% instance, although a recent work (ASPLOS2013) shows that they can parallize 
% data race detection by 4X after applying some smart optimizations, they 
% admits 
% that the detection still has a 5X to 15X slowdown with their benchmarks.


% Worse, existing asynchronous frameworks significantly trade off transparency 
% between the analyses and the frameworks. For better performance, they 
% significantly orchest the frameworks with specific type of analysis, which 
% requires reconstruct from the analysis (two race detection algorithms) into 
% three phases in their framworks, although the analysis algorithms themselves 
% are already notouriously complicated and subtle. Multithreading poses 
% significant performance and transparency issues to existing synchronous 
% frameworks, too (\eg, ShadowReplica, LIFT).



% P2: Existing analysis frameworks have two problems because they are 
% synchronous. Reliability, Securityk, and Profling tools.
% First, slow. Second, can support only one analysis at the same time. Three, 
% % need to orchest the analysis algorithm as well as the framework to adapt to 
% the framework.
% 1: slow.

% 2: can not support mulitple analysis at the same time.
%   Can valgrind do multiple analysis at the same time?
%   Can Speck do multiple analysis at the same time?

% 3: need to orchest the analysis significantly, or even sacrifice guarantees.
%   Peter Chen's: need to separate anaysis to multiple stages.
%   TaintDroid: sacrifice control flow taint.  
% Unfortunately, despite decades of effort, existing program analysis 
% frameworks 
% % are still extremely difficult to deploy in production runs, mainly due to 
% two 
% % % problems. First, these frameworks prohibitively slow down the actual 
% % executions % (\eg, up to 40X in Valgrind and 12X in ShadowReplica), because 
% the % synchronous % approach in these frameworks need to frequently exchange 
% program % states between % the executions and the analysis tools. Whenever a 
% piece of % analysis work is % slow, the actual execution has to wait for the 
% analysis to be % done.
% 
% Second, the synchronous approach causes existing frameworks to significantly 
% involve with the program state exchange with the analysis tool, causing these 
% frameworks to support only one analysis tool at the same time. To the best of 
% our knowledge, no evaluation in existing frameworks have shown to be able to 
% support multiple analysis tools. This problem leads to a paradox: if an 
% application wants the benefit of one analysis tool, it has to exclude the 
% other % tools.

% P3: Why existing sync approach must fail.
% We argue that the synchronous approach in existing frameworks is not 
% fundamental, and many anlysis tools (\eg, data race detectors and profiling 
% tools) can be ran asynchronously, if there is we could meet these 
% requirements: (1) there is a transparent replication framework that can 
% replicate inputs and nondeterministic events such as threads aquiring locks; 
% and (2) there is a mechanism for a analysis tool to notify the framework if 
% bad % events such as deferencing a null pointer occurs or the tool detects 
% that % a % stack overflow occurs; (3) the framework has a transparent 
% application % checkpoint and recovery feature if bad events occurs. Even for 
% a % security % analysis tool, if the security threat does not involve leaking 
% information to % the outside world (\eg, using deallocated memory or 
% uninitialized memory), % which application checkpoint can not revert, this 
% asynchronous approach could % be suitable for this tool.


% P4: Our key insight is that many analysis does not have to be synchronous, 
% and % they can be implemented asynchrously. This paper presents asynchronous 
% analysis % fvalgrind --tool=memcheck ls -lramework. Key weapon, transparent
% state machine replication.

%% Existing types of analysis frameworks:
% 1: shadow memory. Valgrind. TaintDroid. YY Zhou's Lift. Pin. DynamoRio.
% 2: record replay. Peter Chen's Aftersight.
% 3: decoupling execution and analysis. ShadowReplica. Speck (also have record 
% replay).

% P5: Application scope. Not for leaking analysis.
% To meet these requiremnts, this paper presents \xxx, a program analysis 
% framework that supports asynchronous and transparent program anlysis tools. 
% The % core of \xxx is \repbox, a state machine replication (or \smr) system 
% that % can % transparently replicate today's general multithreaded programs 
% for % high % availability. \smr runs replicas of an application on multiple 
% machines % (nodes), tolerating many possible node and network failures.  To 
% keep % the % replicas consistent, it invokes a distributed consensus protocol 
% \paxos~\cite{paxos}) to ensure that a majority of the replicas agree on the 
% input request sequence. \repbox incorporates an efficient Deterministic 
% Multithreading (or \dmt) engine, which practically enforces the same thread 
% interleavings for the application running across replicas.
% 
% \xxx addresses both the two problems in today's analysis frameworks. A 
% typical % setting of a \xxx deployment contains 2\v{f}+1 nodes. A node acts 
% as % the % primary node, which accepts client requests, invokes \paxos 
% consensus on % the % order of these requests, and sends back responses to 
% clients. The other % nodes % act as backup nodes, which reach consensus on 
% requests and process % requests. % One or multiple analysis tools run on at 
% most % \v{f} nodes, while the % other % nodes run the application without 
% analysis and % process requests fast.
% 
% However, to make \xxx's asynchronous analysis framework practical, two 
% practical challenges must be addressed. First, depending on machine restarts 
% and network partitions, a node's role may change, not only between primary 
% and % backup, but also between normal nodes and analysis nodes, ande even 
% between % different analysis nodes. We design a new checkpoint mechanism that 
% matches up % checkpoints from different role machines with input total order 
% ID % to address % this problem. Second, we need a mechanism to roll back all 
% replicas % when a bad % event (\eg, deferencing a NULL pointer) has occured 
% and % detected by % the % analysis tool. We design a a simple API for 
% analysis % tool % to invoke so % that the % replicas can reach consensus on 
% this % roll back % and % act consistently.
% 
% To verify that \xxx's asynchronous approach is feasible, we have evaluated 
% \xxx with four diverse types of widely uses server programs. We measured the 
% checkpint time cost, run multiple valgrind analysis on different nodes, and 
% collected performance results. These results also show that \xxx is 
% complementary to existing analysis frameworks.

% Problem: for some security attack such as stack overflow, the attacker 
% may totally replace the LD_PRELOAd library so that our interception layer
% can never go back, and we can never roll back.

% P6: Key technical challenges. Transparent and timely (to respond to attacks) 
% checkpoint. Migration of analysis from one machine to another.
% When bad events are detected, how does the analysis involve with the 
% framework?
% Notify the framework, involve sync point. Roll back the replicas.

% P7: Our initial results. Checkpoint time. Run with valgrind one analysis. Or 
% two valgrind analysis, one at each replica.
