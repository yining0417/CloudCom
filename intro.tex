\section{Introduction} \label{sec:intro}


% P1: Analysis framework is good and critical, everywhere.
Dynamic program analysis frameworks are pervasive because they enable a wide 
range of powerful runtime analysis tools, including reliability, security, and 
profliing. We classify existing frameworks into three categories: (1) shadow 
memory, (2) record-replay, and (3) decoupling execution and analysis. These 
frameworks take an \emph{synchronous} approach, which frequently exchange an 
application's execution states with the analysis tools. These frameworks have 
embraced various analysis tools, which have shown to find harmfule bugs in 
real-world software and/or prevent serious attacks.



% P2: Existing analysis frameworks have two problems because they are 
% synchronous. Reliability, Securityk, and Profling tools.
% First, slow. Second, can support only one analysis at the same time. Three, 
% % need to orchest the analysis algorithm as well as the framework to adapt to 
% the framework.
% 1: slow.

% 2: can not support mulitple analysis at the same time.
%   Can valgrind do multiple analysis at the same time?
%   Can Speck do multiple analysis at the same time?

% 3: need to orchest the analysis significantly, or even sacrifice guarantees.
%   Peter Chen's: need to separate anaysis to multiple stages.
%   TaintDroid: sacrifice control flow taint.  
Unfortunately, despite decades of effort, existing program analysis frameworks 
are still extremely difficult to deploy in production runs, mainly due to two 
problems. First, these frameworks prohibitively slow down the actual executions 
(\eg, up to 40X in Valgrind and 12X in ShadowReplica), because the synchronous 
approach in these frameworks need to frequently exchange program states between 
the executions and the analysis tools. Whenever a piece of analysis work is 
slow, the actual execution has to wait for the analysis to be done.

Second, the synchronous approach causes existing frameworks to significantly 
involve with the program state exchange with the analysis tool, causing these 
frameworks to support only one analysis tool at the same time. To the best of 
our knowledge, no evaluation in existing frameworks have shown to be able to 
support multiple analysis tools. This problem leads to a paradox: if an 
application wants the benefit of one analysis tool, it has to exclude the other 
tools.

% P3: Why existing sync approach must fail.
We argue that the synchronous approach in existing frameworks is not 
fundamental, and many anlysis tools (\eg, data race detectors and profiling 
tools) can be ran asynchronously, if there is we could meet these 
requirements: (1) there is a transparent replication framework that can 
replicate inputs and nondeterministic events such as threads aquiring locks; 
and (2) there is a mechanism for a analysis tool to notify the framework if bad 
events such as deferencing a null pointer occurs or the tool detects that a 
stack overflow occurs; (3) the framework has a transparent application 
checkpoint and recovery feature if bad events occurs. Even for a security 
analysis tool, if the security threat does not involve leaking information to 
the outside world (\eg, using deallocated memory or uninitialized memory), 
which application checkpoint can not revert, this asynchronous approach could 
be suitable for this tool.


% P4: Our key insight is that many analysis does not have to be synchronous, 
% and % they can be implemented asynchrously. This paper presents asynchronous 
% analysis % fvalgrind --tool=memcheck ls -lramework. Key weapon, transparent
% state machine replication.

%% Existing types of analysis frameworks:
% 1: shadow memory. Valgrind. TaintDroid. YY Zhou's Lift. Pin. DynamoRio.
% 2: record replay. Peter Chen's Aftersight.
% 3: decoupling execution and analysis. ShadowReplica. Speck (also have record 
% replay).

% P5: Application scope. Not for leaking analysis.
To meet these requiremnts, this paper presents \xxx, a program analysis 
framework that supports asynchronous and transparent program anlysis tools. The 
core of \xxx is \repbox, a state machine replication (or \smr) system that can 
transparently replicate today's general multithreaded programs for high 
availability. \smr runs replicas of an application on multiple machines 
(nodes), tolerating many possible node and network failures.  To keep the 
replicas consistent, it invokes a distributed consensus protocol 
\paxos~\cite{paxos}) to ensure that a majority of the replicas agree on the 
input request sequence. \repbox incorporates an efficient Deterministic 
Multithreading (or \dmt) engine, which practically enforces the same thread 
interleavings for the application running across replicas.

\xxx addresses both the two problems in today's analysis frameworks. A typical 
setting of a \xxx deployment contains 2\v{f}+1 nodes. A node acts as the 
primary node, which accepts client requests, invokes \paxos consensus on the 
order of these requests, and sends back responses to clients. The other nodes 
act as backup nodes, which reach consensus on requests and process requests. 
One or multiple analysis tools run on at most \v{f} nodes, while the other 
nodes run the application without analysis and process requests fast.

However, to make \xxx's asynchronous analysis framework practical, two 
practical challenges must be addressed. First, depending on machine restarts 
and network partitions, a node's role may change, not only between primary and 
backup, but also between normal nodes and analysis nodes, ande even between 
different analysis nodes. We design a new checkpoint mechanism that matches up 
checkpoints from different role machines with input total order ID to address 
this problem. Second, we need a mechanism to roll back all replicas when a bad 
event (\eg, deferencing a NULL pointer) has occured and detected by the 
analysis tool. We design a a simple API for analysis tool to invoke so that the 
replicas can reach consensus on this roll back and act consistently.

To verify that \xxx's asynchronous approach is feasible, we have evaluated 
\xxx with four diverse types of widely uses server programs. We measured the 
checkpint time cost, run multiple valgrind analysis on different nodes, and 
collected performance results. These results also show that \xxx is 
complementary to existing analysis frameworks.

% Problem: for some security attack such as stack overflow, the attacker 
% may totally replace the LD_PRELOAd library so that our interception layer
% can never go back, and we can never roll back.

% P6: Key technical challenges. Transparent and timely (to respond to attacks) 
% checkpoint. Migration of analysis from one machine to another.
% When bad events are detected, how does the analysis involve with the 
% framework?
% Notify the framework, involve sync point. Roll back the replicas.

% P7: Our initial results. Checkpoint time. Run with valgrind one analysis. Or 
% two valgrind analysis, one at each replica.
