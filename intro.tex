\section{Introduction} \label{sec:intro}

% P1: virtualization is good
By allowing multiple servers to be consolidated on a small number of physical hosts 
and simplifying provisioning, virtualization is widely used in computing environments 
of various kinds and scales.

% P2: However, previous approaches still fail to efficiently and reliably provide fault-tolerance in virtualized environment
% checkpoint-recovery based replication suffers from two problems
% 1. performance degradation due to the large amount of state that needs to be synchronized between the primary and the backup machines
% 2. significant VM downtimes
% 3. network delay
Nevertheless, server consolidation exacerbates the consequence of unexpected host failures. 
When VMs are consolidated, failure of a single host may bring down mutiple VMs on the host 
and all applications running thereon, resulting in an unacceptable aggregate loss. 
To accommdote virtual machines with high availability, various approaches have been 
proposed to replicate VMs between hosts continuously throughout VM's execution, but 
they are still unable to ensure fault-tolerance in virtualized environments efficiently 
and reliably. One typical technique is checkpoint-recovery based replication of virtual 
machines. It captures the entire execution state of the running VM at relatively 
high frequency so that changes can be reflected to the backup machine nearly instantly. 
One disadvantage of the checkpoint-recovery based replication is the amount of data that 
needs to be copied from the primary to the backup, which can, in some cases, seriously 
affect the performance. Apart from that, VM downtimes incurred by the checkpoint mechanism 
can be significant.

% P3: derterministic replay is the other approach which can address some of the drawbacks of checkpoint-recovery based replication
% single point of failure
% slow for multi-processor

% To answer Heming's question: old leader and new leader get to an inconsistent state
% After recovery, the old leader should be started in the state as the new leader before participating

% 6.824 2016 mit
% What if the network between primary/backup fails?
%  -Primary is still running
%  -Backup becomes a new primary
%  -Two primaries at the same time!
% one can avoid split brain using a single "master"
% master computer decides whether replica A or replica B is the primary
%   there's just one master, so it never disagrees with itself
% clients talk to the master
% this is probably what VMware FT does (atomic test-and-set in shared disk)
% but what if the master fails?
%   it's a "single point of failure" -- not so good
% VMware FT shared disk atomic test-and-set was (apparently) not replicated

% In the non-shared-disk configuration, there may be no shared storage to use for dealing with a split-brain situation.
% In this case, the system could use some other external tiebreaker, such as a third-party server that both servers can talk to.
% If the servers are part of a cluster, the system could alternatively use a majority algorithm based on cluster membership.

On the other hand, derterministic replay is an attractive technique which can address the 
aforementioned drawbacks of checkpoint-recovery based replication: primary and backup execute 
the same sequence of instructions, with non-deterministic events injected exactly at the same 
point into both replicas. However, in the existing works, in order to avoid split brain, 
derterministic replay has to suffer from "single point of failure".

Fortunately, \paxos, a majorty vote algorithm, can be used to prevent the split-brain scenarios 
across the cluster. Moreover,  \paxos provides another great benefit: it ensures the same sequence 
of input requests for replicas. But \paxos is notoriously slow because each decision takes at least 
three message delays between when a replica proposes a command and when some replica learns which 
command has been chosen.

Luckily, Remote Direct Memory Access (RDMA)-capable networks have dropped in price and made 
substantial inroads into datacenters. It allows one computer to directly access the memory of 
a remote computer without involving the operating system at any host. This enables zero-copy 
transfers, reducing latency and CPU overhead.

The remainder of the paper is organized as follows.
