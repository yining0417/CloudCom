\section{Introduction} \label{sec:intro}



% P1: dynamic program analysis framework is good. Traditional ones are 
% fully-coupled, but slow.
Dynamic program analysis frameworks greatly improve software quality as they
enable a wide range of powerful analysis tools (\eg, data race
detectors~\cite{tsan, valgrind:pldi, wester:parallelizing:asplos13} for
multithreaded applications) at runtime. Existing analysis frameworks can be
classified into two approaches depending on how a framework transfers an
application's execution states to an analysis tool. Traditional analysis
frameworks~\cite{dynamorio, pin:pldi05, valgrind:pldi, lift:micro06, tsan} take
a ``fully-coupled" approach: a framework in-lines an analysis with the
execution, then the analysis can inspect all or most execution states. However,
this approach can easily let the analysis slow down the actual execution. To
improve performance, leveraging a fact that many analyses can be done
asynchronously with the actual execution, some recent
frameworks~\cite{decouple:usenix08, speck:asplos08, 
shadowreplica:ccs13, wester:parallelizing:asplos13, superpin, jungwoo:oopsla09}
take a second ``partially-decoupled" approach: only analysis-critical execution
states (\eg, effective memory addresses and thread interleavings) are
transfered to the analysis running on other CPU cores, then the actual execution
can be less perturbed by the analysis.


% The second approach 
% is ``partially-decoupled" approach: to  improve performance, only a subset of 
 % frameworks because powerfuly analysis tend to be heavyweight and 
% prohibitively % slow down the execution.


Unfortunately, despite these great effort, existing analysis frameworks are
still hard to deploy in production runs, mainly due to three problems. The first
problem is still performance. Traditional frameworks fully couples the analysis
with the actual execution using the shadow memory approach, so whenever an
analysis does some heavyweight work, the execution is slowed down (\eg, a
popular race detection tool ThreadSanitizer~\cite{tsan}, which uses a
traditional framework, incurs 20X$\sim$100X slowdown for many programs). Recent
frameworks that take the ``partially-decoupled" approach have shown to run
4X$\sim$8X times faster~\cite{shadowreplica:ccs13,
wester:parallelizing:asplos13} than traditional frameworks because they transfer
fewer 
execution states. However, the overall slowdown of these recent frameworks are
still prohibitive in their own evaluation because the amount of execution states
(\eg, effective memory addresses~\cite{shadowreplica:ccs13} and thread
interleavings~\cite{wester:parallelizing:asplos13}) transfered to analysis tools
are still enormous.


The second problem is that recent frameworks with the ``partially-decoupled"
approach heavily trade off transparency with analysis tools. To be able to
transfer fewer execution states to an analysis tool, these frameworks require
heavily carving the analysis tool as well as the transfered execution states.
For example, a recent framework~\cite{wester:parallelizing:asplos13} for race
detection tools leverages the record-replay
technique~\cite{scribe:sigmetrics2010, respec:asplos10, racepro:sosp11} to
reduce analysis work in the actual execution, but this framework
requires significantly carving race detection tools into several phases to adapt
to record-replay, which makes the tools largely different from typical ones
(Note that an analysis tool itself is already quite difficult to implement and
maintain.)


The third problem is that existing frameworks, including both traditional ones
and recent ones, have not shown to support multiple types of analysis tools
within one execution. Multiple analysis tools bring multiple powerful
guarantees, which is attractive to today's applications. However, the
traditional ``fully-coupled" frameworks typically take the shadow memory
approach to hold analysis results for each memory byte in the actual execution,
then different analyses may require different copies of shadow memory per byte.
It is not trivial to extend these frameworks to support multiple copies of
shadow memory. Considering ``partially-decoupled" frameworks, it is not trivial
to extend them to support multiple types of analysis tools within one execution
either, because these frameworks heavily carve the transfered execution states,
which are hard to reuse for different types of analysis tools.


In sum, a fundamental reason for the three problems in existing analysis 
frameworks is that they run only one actual execution, so an analysis tool has
to be fully or partially coupled with the execution in order to inspect
execution states. Well, what if one can construct multiple equivalent
executions efficiently and transparently?




% P4: Our approach. Replication. DMT. And why we are better than existing 
% replication systems: avoid shipping schedules, avoid annotations.
To address this question, this paper presents \xxx, an efficient, 
transparent dynamic program analysis framework by leveraging a technique called 
\emph{state machine replication (or \smr)}~\cite{paxos:simple, 
paxos:practical, paxos}. \smr assumes a deterministic application, runs the 
application on multiple machines (or replicas), and typically uses the \paxos 
protocol to enforce a consistent sequence of inputs for the application across 
replicas as long as a majority of replicas reach consensus on each input. 
Leveraging \smr, all replicas consistently transform the same execution states, 
then minor replicas can just transparently run an heavyweight analysis tool on 
each, and majority replicas run actual executions or lightweight analysis tools 
to process inputs fast.


However, \smr alone is not sufficient to construct multiple equivalent 
executions because today's applications widely use threads and thus are 
nondeterministic. At runtime, replicas may run into different thread
interleavings (or \emph{schedules}), easily leading to divergent execution
states. To address this problem, \xxx also leverages \emph{deterministic
multithreading (or \dmt)}, a recent advanced threading technique that
efficiently enforce inter-thread communications to happen in the same total
order, then different replicas run the same schedules on the same input and
transform the same execution states.


A practical challenge for \xxx is that even asynchronous analysis tools
sometimes would prefer to roll back to previous execution states and discard 
malicious inputs that triggered harmful events. To address this challenge, \xxx 
has incorporated a transparent application-level checkpoint mechanism with 
expressive API for replicas to rock back consistently.


We have conducted a feasibility study for \xxx in Linux. To leverage \smr, \xxx 
incorporates a \paxos protocol implementation in \repbox~\cite{repbox:sosp15}, 
a transparent \smr system for general server applications. To leverage \dmt, 
\xxx incorporates \parrot, a \dmt runtime system that has shown to run fast on 
a wide range of popular multithreaded programs. Evaluation on a popular 
parallel anti-virus scanning server \clamav~\cite{clamav} with three replicas 
(each has 24 cores) shows that \xxx is able to transparently run one 
heavyweight analysis tool, the Helgrind race detector~\cite{valgrind:pldi}, and 
one lightweight analysis tool, DynamoRio's code coverage tool 
drcov~\cite{dynamorio}, within the same execution. Moreover, \xxx incurred 
merely 2.1\% overhead over the actual execution.


% P6: Benefits. This paragraph can be removed.
The main contribution of \xxx is the idea of constructing multiple equivalent
executions to fully decouple an application's actual execution and analysis
tools, which benefits applications, analysis tools, and frameworks. \xxx can
greatly improve an application's quality by running multiple analysis tools in
production runs efficiently. \xxx is transparent to analysis tools and has the
potential to strengthen and speedup the tools themselves. \xxx compensates 
traditional ``fully-coupled" frameworks because it can easily run them on the 
analysis replicas. Notably, \xxx's main purpose is \emph{not} to provide fault 
tolerance, but to fully decouple actual executions and analysis tools.

In the remaining of this paper, \S\ref{sec:background} introduces the 
background of \smr and \dmt, \S\ref{sec:overview} presents an overview \xxx's 
design, \S\ref{sec:discuss} discusses \xxx's potential benefits, and 
\S\ref{sec:checkpoint} describes \xxx's checkpoint and rollback mechanism. 
\S\ref{sec:eval} presents evaluation results, \S\ref{sec:related} introduces 
related work, and \S\ref{sec:conclusion} concludes.