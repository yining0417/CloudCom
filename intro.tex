\section{Introduction} \label{sec:intro}



% P1: dynamic program analysis framework is good. Traditional ones are 
% fully-coupled, but slow.
Dynamic program analysis frameworks have become increasingly pervasive and 
critical because they enable a wide range of powerful analysis tools at 
runtime, including reliability, profiling and logging tools. To let analysis 
easily inspect an application's actual execution, traditional program analysis 
frameworks inlines the execution with analysis (\eg, Valgrind, DynamoRio, Pin, 
LIFT), and we call these frameworks's approach the ``fully-coupled 
approach". Performance is a major problem for these frameworks because 
powerfuly analysis tend to be heavyweight and prohibitively slow down the 
execution.


Leveraging a fact that many analyses can be done asynchronously with the 
executions, some recent frameworks only transfert a subset of application 
execution states (\eg, effective memory addresses, thread interleavings) 
between the execution and the analysis. We call this approach the 
``partially-decoupled approach". Because these frameworks transfer fewer 
states, they can significantly speedup the actual executions (\eg, 
ShadowReplica and ASPLOS2013). However, since the execution and the analysis 
are still partially coupled, the resultant overhead of these frameworks is 
still high (\eg, XX for ShadowReplica and YY for ASPLOS2013).


Worse, to maximize speedup, existing frameworks that take the 
partially-decoupled approach heavily tune execution states exchanged between 
the execution and the analysis, which requires significantly reconstructing 
analysis tools as well as the frameworks themselves. For example, regarding the 
execution states between the execution and the analysis, a recent data flow 
tracking framework carefully selects only effective computed memory addresses 
and branches as the execution states, and a recent data race detection 
framework selects the vector clocks of synchronization operationss. These 
tunings not only heavily trade off transparency between the 
analysis and the framework (note an analysis itself is often already extremely 
complicated to maintain), but also resticts the framework to run only one type 
of analyses at the same time.

% Existing in-line 
% analysis frameworks also only supports one analysis at the same time because 
% a % typical such frameworks typically maintain analysis results with shadow 
% memory, 
% which can only support one analysis in one execution.


We argue that many analyses can be fully decoupled from the executions if 
we could have such a framework that can meet three requirements. First (R1), 
the framework must consistently replicate the same inputs for the executions 
with and without analysis even for server applications (\eg, \apache) that 
continuously accept inputs. Second (R2), given the same input, the framework 
must efficiently enforce the same thread interleavings (or \emph{schedules}) 
for execution and analysis without the need of frequently transfering schedules 
between them. Third (R3), when bad events occur in the execution or the 
analysis (\eg, a NULL pinter dereference), the framework must provide a 
transparent checkpoint and restore mechanism to recover both the execution and 
the analysis back to an equavalent good state.


% P4: Our approach. Replication. DMT. And why we are better than existing 
% replication systems: avoid shipping schedules, avoid annotations.
To meet these three requirments, this paper presents \xxx, an efficient, 
transparent program analysis framework that can fully decouple execution and 
analysis by leveraging two techniques: state machine replication (or \smr) and 
deterministic multithreading (or \dmt). \xxx runs a total number of 2\v{f}+1 
nodes, while \v{f}+1 nodes run the actual executions, and \v{f} nodes run one 
analysis on each. To meet R1, as a typical \smr setting to keep the replicas 
processing the same sequence of input requests, \xxx incorporates a distributed 
consensus protocol \paxos~\cite{paxos} so that as long as a majority of nodes 
agree on a new request, all replicas comply with this agreement. To meet R2, 
\dmt runs in each replica and enforces that all inter-thread communications 
happen in the same order across replicas without synchronizing schedules between 
the exeuctions running with and without analysis. Recent 
study~\cite{parrot:sosp13} has shown that \dmt runs fast on a 
wide range of 108 popular multithreaded programs on 24-core machines. To meet 
R3, we design a checkpoint mechanism that matches up checkpoints from different 
role machines with input total order ID.


% P5: A practical challenge. Transparent checkpoint.
% However, to make \xxx's asynchronous analysis framework practical, two 
% challenges should be addressed. First, depending on machine restarts and 
% network partitions, a node's role may change, not only between a \paxos 
% primary 
% and backup, but also between normal nodes and analysis nodes, ande even 
% between 
% different analysis nodes. We design a checkpoint mechanism that matches up 
% checkpoints from different role machines with input total order ID to address 
% this challenge. Second, we need a mechanism to roll back all replicas when a 
% bad event (\eg, deferencing a NULL pointer) has occured and detected by the 
% analysis tool. We design a a simple API for analysis tool to invoke so that 
% the 
% replicas can reach consensus on this roll back and act consistently.


We have implemented a \xxx system propotype in Linux. Evaluation on four widely 
used server programs show that our system runs efficiently with two heavyweight 
Valgrind analyses running on one replica with reasonable overhead XXX\%, and 
our checkpoint mechanism brings negligible overhead to the overall system, 
transparent to the analysis, and is robust to replica restarts.


% P6: Benefits. This paragraph can be removed.
The key contribution of \xxx is the idea of fully decoupling execution 
and analysis, which introduces three major benefits. First, \xxx's replication 
architecture provides an efficient framework for many asynchronous 
analysis tools, ranging from reliability tools, to profiling tools, and to 
logging tools. Second, unlike many previous work which tightly orchest the 
framework with the analysis for better performance (\eg, ShadowReplica and 
ASPLOS13), our framework is transparent to analysis logic, so it is 
complementary to existing analyses as well as frameworks and can easily 
deploy existing frameworks in our replicas (\eg, Valgrind). Third, the 
flexibility on the number of replicas enable multiple analyses (up to \v{f}) in 
one execution.






% Powerfuly analyses are usually heavyweight and prohibitively slow down the 
% actual execution that runs synchronously. This problem has restricted most of 
% existing frameworks as well as the analysis tools to deploy only in testing 
% labs without being able to exercise real-world workloads. In fact, many 
% analyses can be done asynchronously (\eg, data race detection and profling 
% tools), so recently researchers have proposed asynchronous frameworks (\eg, 
% Aftersight, Spec, ASPLOS13) via an advanced technique called 
% ``record-replay": % the record phase runs the actual execution and records 
% nondeterministic sources % such as inputs, while the replay phase runs 
% (typically offline) the actual % execution with an analysis tool on the 
% recorded inputs. This asynchronous % approach decouples the analysis from the 
% execution and lets the actual % execution run efficiently.


% P3: Some recent work tried to avoid the synchronizing execution states with
% an approach called record-replay. But thread interleabings still need to sync.
% and not transparent as well.
% Unfortunately, the emerging multithreading trend, which is driven by the rise 
% of the multi-core hardware, poses a significant challenge on existing 
% asynchronous frameworks. For instance, today's popular server applications 
% (\eg, \apache) use multiple threads to server requests parallelly. These 
% parallel and continuously running applications require record-replay to run 
% online (cite Respec) which frequently synchronize thread interleavings (or 
% \emph{schedules}), otherwise the record and replay may run into different 
% schedules and easily leading to divergent execution states. Despite a variety 
% of notable approaches are proposed (cite Respec, DoublePlay, and ASPLOS 
% 2013), 
% efficient and scalable online record-replay is still an open problem. For 
% instance, two notable systems incur 28\% to 55\% overhead with four threads 
% in 
% record phase.

% For 
% instance, although a recent work (ASPLOS2013) shows that they can parallize 
% data race detection by 4X after applying some smart optimizations, they 
% admits 
% that the detection still has a 5X to 15X slowdown with their benchmarks.


% Worse, existing asynchronous frameworks significantly trade off transparency 
% between the analyses and the frameworks. For better performance, they 
% significantly orchest the frameworks with specific type of analysis, which 
% requires reconstruct from the analysis (two race detection algorithms) into 
% three phases in their framworks, although the analysis algorithms themselves 
% are already notouriously complicated and subtle. Multithreading poses 
% significant performance and transparency issues to existing synchronous 
% frameworks, too (\eg, ShadowReplica, LIFT).



% P2: Existing analysis frameworks have two problems because they are 
% synchronous. Reliability, Securityk, and Profling tools.
% First, slow. Second, can support only one analysis at the same time. Three, 
% % need to orchest the analysis algorithm as well as the framework to adapt to 
% the framework.
% 1: slow.

% 2: can not support mulitple analysis at the same time.
%   Can valgrind do multiple analysis at the same time?
%   Can Speck do multiple analysis at the same time?

% 3: need to orchest the analysis significantly, or even sacrifice guarantees.
%   Peter Chen's: need to separate anaysis to multiple stages.
%   TaintDroid: sacrifice control flow taint.  
% Unfortunately, despite decades of effort, existing program analysis 
% frameworks 
% % are still extremely difficult to deploy in production runs, mainly due to 
% two 
% % % problems. First, these frameworks prohibitively slow down the actual 
% % executions % (\eg, up to 40X in Valgrind and 12X in ShadowReplica), because 
% the % synchronous % approach in these frameworks need to frequently exchange 
% program % states between % the executions and the analysis tools. Whenever a 
% piece of % analysis work is % slow, the actual execution has to wait for the 
% analysis to be % done.
% 
% Second, the synchronous approach causes existing frameworks to significantly 
% involve with the program state exchange with the analysis tool, causing these 
% frameworks to support only one analysis tool at the same time. To the best of 
% our knowledge, no evaluation in existing frameworks have shown to be able to 
% support multiple analysis tools. This problem leads to a paradox: if an 
% application wants the benefit of one analysis tool, it has to exclude the 
% other % tools.

% P3: Why existing sync approach must fail.
% We argue that the synchronous approach in existing frameworks is not 
% fundamental, and many anlysis tools (\eg, data race detectors and profiling 
% tools) can be ran asynchronously, if there is we could meet these 
% requirements: (1) there is a transparent replication framework that can 
% replicate inputs and nondeterministic events such as threads aquiring locks; 
% and (2) there is a mechanism for a analysis tool to notify the framework if 
% bad % events such as deferencing a null pointer occurs or the tool detects 
% that % a % stack overflow occurs; (3) the framework has a transparent 
% application % checkpoint and recovery feature if bad events occurs. Even for 
% a % security % analysis tool, if the security threat does not involve leaking 
% information to % the outside world (\eg, using deallocated memory or 
% uninitialized memory), % which application checkpoint can not revert, this 
% asynchronous approach could % be suitable for this tool.


% P4: Our key insight is that many analysis does not have to be synchronous, 
% and % they can be implemented asynchrously. This paper presents asynchronous 
% analysis % fvalgrind --tool=memcheck ls -lramework. Key weapon, transparent
% state machine replication.

%% Existing types of analysis frameworks:
% 1: shadow memory. Valgrind. TaintDroid. YY Zhou's Lift. Pin. DynamoRio.
% 2: record replay. Peter Chen's Aftersight.
% 3: decoupling execution and analysis. ShadowReplica. Speck (also have record 
% replay).

% P5: Application scope. Not for leaking analysis.
% To meet these requiremnts, this paper presents \xxx, a program analysis 
% framework that supports asynchronous and transparent program anlysis tools. 
% The % core of \xxx is \repbox, a state machine replication (or \smr) system 
% that % can % transparently replicate today's general multithreaded programs 
% for % high % availability. \smr runs replicas of an application on multiple 
% machines % (nodes), tolerating many possible node and network failures.  To 
% keep % the % replicas consistent, it invokes a distributed consensus protocol 
% \paxos~\cite{paxos}) to ensure that a majority of the replicas agree on the 
% input request sequence. \repbox incorporates an efficient Deterministic 
% Multithreading (or \dmt) engine, which practically enforces the same thread 
% interleavings for the application running across replicas.
% 
% \xxx addresses both the two problems in today's analysis frameworks. A 
% typical % setting of a \xxx deployment contains 2\v{f}+1 nodes. A node acts 
% as % the % primary node, which accepts client requests, invokes \paxos 
% consensus on % the % order of these requests, and sends back responses to 
% clients. The other % nodes % act as backup nodes, which reach consensus on 
% requests and process % requests. % One or multiple analysis tools run on at 
% most % \v{f} nodes, while the % other % nodes run the application without 
% analysis and % process requests fast.
% 
% However, to make \xxx's asynchronous analysis framework practical, two 
% practical challenges must be addressed. First, depending on machine restarts 
% and network partitions, a node's role may change, not only between primary 
% and % backup, but also between normal nodes and analysis nodes, ande even 
% between % different analysis nodes. We design a new checkpoint mechanism that 
% matches up % checkpoints from different role machines with input total order 
% ID % to address % this problem. Second, we need a mechanism to roll back all 
% replicas % when a bad % event (\eg, deferencing a NULL pointer) has occured 
% and % detected by % the % analysis tool. We design a a simple API for 
% analysis % tool % to invoke so % that the % replicas can reach consensus on 
% this % roll back % and % act consistently.
% 
% To verify that \xxx's asynchronous approach is feasible, we have evaluated 
% \xxx with four diverse types of widely uses server programs. We measured the 
% checkpint time cost, run multiple valgrind analysis on different nodes, and 
% collected performance results. These results also show that \xxx is 
% complementary to existing analysis frameworks.

% Problem: for some security attack such as stack overflow, the attacker 
% may totally replace the LD_PRELOAd library so that our interception layer
% can never go back, and we can never roll back.

% P6: Key technical challenges. Transparent and timely (to respond to attacks) 
% checkpoint. Migration of analysis from one machine to another.
% When bad events are detected, how does the analysis involve with the 
% framework?
% Notify the framework, involve sync point. Roll back the replicas.

% P7: Our initial results. Checkpoint time. Run with valgrind one analysis. Or 
% two valgrind analysis, one at each replica.
