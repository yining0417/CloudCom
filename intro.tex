\section{Introduction} \label{sec:intro}



% P1: dynamic program analysis framework is good. Traditional ones are 
% fully-coupled, but slow.
Dynamic program analysis frameworks greatly improve software quality as they
enable a wide range of powerful analysis tools (\eg, data race
detectors~\cite{tsan, valgrind:pldi, wester:parallelizing:asplos13} for
multithreaded applications) at runtime. Existing analysis frameworks can be
classified into two approaches depending on how a framework transfers an
application's execution states to an analysis tool. Traditional analysis
frameworks~\cite{dynamorio, pin:pldi05, valgrind:pldi, lift:micro06, tsan} take
a ``fully-coupled" approach: a framework in-lines an analysis with the
execution, then the analysis can inspect all or most execution states. However,
this approach can easily let the analysis slow down the actual execution. To
improve performance, leveraging a fact that many analyses can be done
asynchronously with the actual execution, some recent
frameworks~\cite{decouple:usenix08, speck:asplos08, 
shadowreplica:ccs13, wester:parallelizing:asplos13, superpin, jungwoo:oopsla09}
take a second ``partially-decoupled" approach: only analysis-critical execution
states (\eg, effective memory addresses and thread interleavings) are
transfered to the analysis running on other CPU cores, then the actual execution
can be less perturbed by the analysis.


% The second approach 
% is ``partially-decoupled" approach: to  improve performance, only a subset of 
 % frameworks because powerfuly analysis tend to be heavyweight and 
% prohibitively % slow down the execution.


Unfortunately, despite these great effort, existing analysis frameworks are
still hard to deploy in production runs, mainly due to three problems. The first
problem is still performance. Traditional frameworks fully couples the analysis
with the actual execution using the shadow memory approach, so whenever an
analysis does some heavyweight work, the execution is slowed down (\eg, a
popular race detection tool ThreadSanitizer~\cite{tsan}, which uses a
traditional framework, incurs 20X$\sim$100X slowdown for many programs). Recent
frameworks that take the ``partially-decoupled" approach have shown to run
4X$\sim$8X times faster~\cite{shadowreplica:ccs13,
wester:parallelizing:asplos13} than traditional frameworks because they transfer
fewer 
execution states. However, the overall slowdown of these recent frameworks are
still prohibitive in their own evaluation because the amount of execution states
(\eg, effective memory addresses~\cite{shadowreplica:ccs13} and thread
interleavings~\cite{wester:parallelizing:asplos13}) transfered to analysis tools
are still enormous.


The second problem is that recent frameworks with the ``partially-decoupled"
approach heavily trade off transparency with analysis tools. To be able to
transfer fewer execution states to an analysis tool, these frameworks require
heavily carving the analysis tool as well as the transfered execution states.
For example, a recent framework~\cite{wester:parallelizing:asplos13} for race
detection tools leverages the record-replay
technique~\cite{scribe:sigmetrics2010, respec:asplos10, racepro:sosp11} to
reduce analysis work in the actual execution, but this framework
requires significantly carving race detection tools into several phases to adapt
to record-replay, which makes the tools largely different from typical ones
(Note that an analysis tool itself is already quite difficult to implement and
maintain.)


The third problem is that existing frameworks, including both traditional ones
and recent ones, have not shown to support multiple types of analysis tools
within one execution. Multiple analysis tools bring multiple powerful
guarantees, which is attractive to today's applications. However, the
traditional ``fully-coupled" frameworks typically take the shadow memory
approach to hold analysis results for each memory byte in the actual execution,
then different analyses may require different copies of shadow memory per byte.
It is not trivial to extend these frameworks to support multiple copies of
shadow memory. Considering ``partially-decoupled" frameworks, it is not trivial
to extend them to support multiple types of analysis tools within one execution
either, because these frameworks heavily carve the transfered execution states,
which are hard to reuse for different types of analysis tools.


In sum, a fundamental reason for the three problems in existing analysis 
frameworks is that they run only one actual execution, so an analysis tool has
to be fully or partially coupled with the execution in order to inspect
execution states. Well, what if one can construct multiple equivalent
executions efficiently and transparently?




% Existing in-line 
% analysis frameworks also only supports one analysis at the same time because 
% a % typical such frameworks typically maintain analysis results with shadow 
% memory, 
% which can only support one analysis in one execution.


% We argue that many analyses can be fully decoupled from the executions if 
% we could have such a framework that can meet three requirements. First (R1), 
% the framework must consistently replicate the same inputs for the executions 
% with and without analysis even for server applications (\eg, \apache) that 
% continuously accept inputs. Second (R2), given the same input, the framework 
% must efficiently enforce the same thread interleavings (or \emph{schedules}) 
% for execution and analysis without the need of frequently transfering 
% schedules % between them. Third (R3), when bad events occur in the execution 
% or % the % analysis (\eg, a NULL pinter dereference) caused by some malicious 
% inputs, % the % framework must provide a transparent checkpoint and restore 
% mechanism to % recover both the execution and the analysis back to an 
% equavalent % state % before % the malicious inputs are processed, and then 
% replicas % consistently % discard the % malicious inputs.


% P4: Our approach. Replication. DMT. And why we are better than existing 
% replication systems: avoid shipping schedules, avoid annotations.
To address this question, this paper presents \xxx, an efficient, 
transparent dynamic program analysis framework by leveraging a technique called 
\emph{state machine replication (or \smr)}~\cite{paxos:simple, 
paxos:practical, paxos}. \smr assumes a deterministic application, runs the 
application on multiple machines (or replicas), and enforces a consistent 
sequence of inputs for the application across replicas as long as a majority of 
replicas reach consensus on each input. Leveraging \smr, all replicas
consistently transform the same execution states, then minor replicas can just
transparently run an analysis tool on each and majority
replicas run actual executions to process inputs fast.


However, \smr alone is not sufficient to construct multiple equivalent 
executions because today's applications widely use threads and thus are 
nondeterministic. At runtime, replicas may run into different thread
interleavings (or \emph{schedules}), easily leading to divergent execution
states. To address this problem, \xxx also leverages \emph{deterministic
multithreading (or \dmt)}, a recent advanced threading technique that
efficiently enforce inter-thread communications to happen in the same total
order, then different replicas run the same schedules on the same input and
transform the same execution states.


A practical challenge for \xxx is that even asynchronous analysis tools
sometimes would prefer to roll back to previous execution states and discard 
malicious inputs that triggered harmful events. To address this challenge, \xxx 
has incorporated a transparent application-level checkpoint mechanism with 
expressive API for replicas to rock back consistently (\S\ref{sec:checkpoint}).


We have conducted a feasibility on \xxx in Linux. To leverage \smr, \xxx 
incoporates the \paxos implementation in \repbox, a transparent \smr 
system for general server applications. To leverage \dmt, \xxx incoporates 
\parrot, a \dmt system that has shown to run fast on a wide range of 
popular multithreaded programs. Evaluation on a widely used multimedia server 
program \mediatomb~\cite{mediatomb} with three replicas (each has 24 cores) 
shows that \xxx is able to support multiple anslysis tools transparently 
without modifying these tools. Moreover, \xxx only incurred 1.8\% overhead over 
its bare replication framework.


% P6: Benefits. This paragraph can be removed.
The main contribution of \xxx is the idea of constructing multiple equivalent
executions to fully decouple an application's actual execution and analysis
tools, which benefits applications, analysis tools, and frameworks. \xxx can
greatly improve an application's quality by running multiple analysis tools in
production runs efficiently. \xxx is transparent to analysis tools and has the
potential to strengthen and speedup the tools themselves (\S\ref{sec:discuss}). 
\xxx compensates traditional ``fully-coupled" frameworks because it can easily 
run them on the analysis replicas.

The rest of this paper, \S\ref{sec:background} introduces the background of 
\smr and \dmt, \S\ref{sec:overview} presents an overview \xxx's design, 
\S\ref{sec:discuss} discusses \xxx's potential benefits, and 
\S\ref{sec:checkpoint} describes \xxx's checkpoint and rollback mechanism. 
\S\ref{sec:eval} presents evaluation results, \S\ref{sec:related} introduces 
related work, and \S\ref{sec:conclusion} concludes.