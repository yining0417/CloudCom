\section{Introduction} \label{sec:intro}

% implement application fault tolerance at the virtual machine level

% P1: virtualization is good
Server virtualization has emerged as a powerful technique for consolidating servers in 
data centers. The reduction of the number of physical hosts also contributes to cutting 
back the power consumptions in the data centers.

Nevertheless, server consolidation exacerbates the consequence of unexpected host failures. 
When VMs are consolidated, failure of a single host may bring down mutiple VMs on the host 
and all applications running thereon, resulting in an unacceptable aggregate loss. 
To realize fault tolerance in virtualized environments, various approaches , but they are
oriented toward protecting the VM. The challenge is that the VM alone does not guarantee uptime 
for applications and services. Detecting and remediating VM failure falls short of what is 
truly vital, detecting and remediating application and service failures.

State Machine Replication (SMR) is an attractive approach to achieving application fault tolerance. 
SMR runs replicas of the program and invokes a distributed consensus protocol 
(typically \paxos) to ensure the same sequence of input requests for replicas, as long as a 
quorum (typically a majority) of the replicas agrees on the input request sequence.

However, \paxos is notoriously slow because each decision takes at least 
three message delays between when a replica proposes a command and when some replica learns which 
command has been chosen.

Fortunately, Remote Direct Memory Access (RDMA)-capable networks have dropped in price and made 
substantial inroads into datacenters. RDMA operations allow a machine to read (or write) from a 
pre-registered memory region of another machine without involving the CPU on the remote side. 
Compared to traditional message passing, RDMA achieves the smallest round-trip latency 
(~$\sim$3 \us), highest throughput, and lowest (zero) CPU overhead~\cite{pilaf:usenix14}.

% To answer Heming's question: old leader and new leader get to an inconsistent state
% After recovery, the old leader should be started in the state as the new leader before participating

% 6.824 2016 mit
% What if the network between primary/backup fails?
%  -Primary is still running
%  -Backup becomes a new primary
%  -Two primaries at the same time!
% one can avoid split brain using a single "master"
% master computer decides whether replica A or replica B is the primary
%   there's just one master, so it never disagrees with itself
% clients talk to the master
% this is probably what VMware FT does (atomic test-and-set in shared disk)
% but what if the master fails?
%   it's a "single point of failure" -- not so good
% VMware FT shared disk atomic test-and-set was (apparently) not replicated

% In the non-shared-disk configuration, there may be no shared storage to use for dealing with a split-brain situation.
% In this case, the system could use some other external tiebreaker, such as a third-party server that both servers can talk to.
% If the servers are part of a cluster, the system could alternatively use a majority algorithm based on cluster membership.


% This paper presents \xxx, a software system that provides high availability

The remainder of the paper is organized as follows.
