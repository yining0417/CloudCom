\section{Introduction} \label{sec:intro}

% P1: lots of applications require various big data frameworks. Trading, fraud 
% detection, aviation, medical, military. But existing systems can not meet the 
% high latency and availability requriements of these applications.
Driven the the drastically increasing computational demands and the volumns of 
data, cluster managemement systems (\eg, Mesos~\cite{mesos:nsdi11}) are 
incorporating more and more applications (\eg, Spark~\cite{spark:nsdi11} and 
\redis~\cite{redis}) in order to harness the computational resources in 
clusters for these applications. Applications not only include typical batch 
frameworks (\eg, Hadoop~\cite{hadoop}), but also critical applications such as 
trading platforms, fraud detection systems, health care systems, and military 
systems. We consider an application \emph{critical} if its high requirements on 
availability and response time. For example, a high-frequency trading platform 
tends to be highly-availabile during the stock operation hours, and adding a few 
hundred \us to the platform's response time means huge money lost.

Unfortunately, despite recent advances in building and applying cluster 
managemement systems~\cite{borg:eurosys15,mesos:nsdi11,tupperware,yarn:socc13,
autopilot:sosp07,quincy:sosp09,apollo:osdi14,fuxi:vldb14}, these systems are 
still difficult to meet the high requirements of critical applications because 
these systems do not provide high-availability to the applications. 
Specifically, to make the systems themselves highly-availabile, existing systems 
typically replicate their controller component which accepts tasks, allocate 
computational resources, and (re)schedule tasks on availabile resources. 
However, the applications themselves are not replicated: if an application 
crashes or a computational resource goes down (\eg, hardware errors), these 
systems have to reschedule the tasks, leaving an arbitral unavailable time 
window for these applications. A possible key reason of this problem is that 
these systems are initially designed for big-data engines which already 
considered fault recovery (but, not availability).

% P2: SMR a promising approach, but slow.
State machine replication (SMR)~\cite{paxos} may be a promising approach 
to address the availability problem of critical applications. SMR runs the 
same program on a number of replicas and uses a distributed consensus protocol 
(\eg, 
\paxos~\cite{paxos:practical,paxos,paxos:simple,paxos:complex,epaxos:sosp13}) 
to enforce the same inputs among 
replicas. An input consensus can achieve as long as a majority of replicas 
agree, thus SMR can tolerate various faults such as minor replica failures. For 
instance, two existing systems \borg~\cite{borg} and \mesos~\cite{borg} use 
\paxos to replicate their controllers.

% Typically, \paxos assigns a replica as the leader to propose 
% consensus requests, and the other replicas agree or reject requests. 


However, \paxos's consensus is notoriously difficult to be fast. 
To agree on an input, traditional consensus protocols invoke at least one 
message round-trip between two replicas. Given that a \v{ping} in Ethernet 
takes hundreds of \us, a program running in an SMR system with three replicas 
must wait at least this time before processing an input. 

% P3: RDMA, new opportunity. Two RDMA systems.
Remote Direct Access Memory (RDMA) is a promising technique to mitigate 
consensus latency because recently it becomes cheaper and increasingly 
pervasive in datacenters. RDMA allows a process to directly write to the 
user space memory of a remote process, completely bypassing the remote OS 
kernel or CPU (the so called ``one-sided" operations). An 
evaluation~\cite{pilaf:usenix14} shows that such a write round-trip takes only 
$\sim$3 \us in the Infiniband architecture~\cite{infiniband}. Two recent 
RDMA-enabled \paxos implementations~\cite{dare:hpdc15,falcon:github} reported 
$\sim$15 \us consensus latency.

A straightforward approach to achieve high-availability for critical 
applications could be integrating \paxos within each application. However, 
doing so faces two issues. First, \paxos is notoriously difficult to 
understand~\cite{raft:usenix14}, implement~\cite{paxos:practical}, and 
verify~\cite{demeter:sosp11}. Second, running a \paxos-enabled application in a 
cluster managemement system could be problematic because the system is unaware 
of the. For instance, if the system schedules all the replicas of the same 
task on the same machine, then a single-point failure on this machine 
will turn down all replicas. 
% Integrating \paxos to an application requires 
% strong \paxos expertise for the application developers

% As a common 
% RDMA practice, to ensure that such a write successfully resides in the 
% memory of a remote process, the local process should wait until the remote NIC 
% (network interface card) sends an ACK to the local host's NIC. 

% P4: We present a new fast, highly-available computing platform by building a 
% eco system with Mesos and RDMA SMR systems.
This paper proposes \xxx, a cluster management system design that automatically 
provides high-availability to general applications. To avoid the two 
aforementioned issues, \xxx's design chooses to integrate \paxos in the cluster 
system, not the application. Doing so has two benefits. First, \xxx's \paxos 
acts a single, general SMR service to general applications: we can just 
leverage existing reliability tools~\cite{modist:nsdi09,demeter:sosp11} to make 
sure this \paxos protocol is correct and then we can benefit many other 
applications. Second, now \xxx's own scheduler can handle the replication logic 
and schedule replicas of the same task on diverse physical machines.

\xxx achieves fast fault-tolerance with RDMA-enabled 
\paxos protocols~\cite{dare:hpdc15,falcon:github}. \xxx supports both normal 
applications (\ie, non-critical) and critical ones. In \xxx, a replicas of 
controllers run a critical application's same task on diverse physical 
machines, and controllers use a fast \paxos protocol to agree on computational 
requests. 

% Benefits.
In an implementation level, \xxx proposes to integrate a RDMA-enabled \paxos 
protocol with \mesos~\cite{mesos}, a widely used cluster managemement system. 
This integration forms a mutual beneficial eco-system\footnote{We name our 
system after the ancient Chinese three-legged tripod, a reliable, multi-purpose 
container.}: by replicating tasks with \paxos, \xxx ensures that minor failures 
in a task does not affect the other tasks to compute reply efficiently. By 
leveraging \mesos's resource allocation and isolation feature, \xxx can run 
diverse applications on the same physical machine, saving much of the phyiscal 
hardware cost in typical \paxos.

% P5: Applications enjoy better latency and fault-tolerance.

% P6: Benefit, now frameworks can focus on their own logic, no longer need 
% fault-toelrance module. Fault-tolerance requires expert knowledge, really 
% hard to build.

% P7: Can also address the performance problem: slow tail.
% A happy side-effect in \xxx is that it 
% addresses a pervasive latency-tail challenge via its fault-tolerance 
% architecture. 

In addition to fault-tolerance, \xxx's design can also mitigate the 
tail-tolerance problem (or straggler)~\cite{tail:scale} in cluster computing. 
The tail of latency distribution problem is pervasive for decades: since a 
job request consists of tasks on diverse machines, the response of this 
job is often bounded by the slowest task. Although \xxx is not the first work 
that presents a replication approach to address the straggler 
problem~\cite{dolly:nsdi13}, \xxx's tail-tolerance design is built on top of its 
\paxos replication achitecture for simplicy and generality.

% We propose a general approach with 
% \xxx's fault-tolerance architecture: \xxx collects replicas' output on a request 
% with its consensus protocol, it then replies to client programs with the 
% fastest replica's result. Therefore, applications can safely ignore this problem 
% and focus on their application logic.

% P8: in essense, a datacenter OS: handles resource allocation/isolation, 
% fault-tolerance, and performance for frameworks (applications). Use 
% replication to bstract away % physical machine details; frameworks now only 
% see computing resources, safely ignore other things such as physical 
% machines, fault-tolerance, and tails.
As part of \xxx's development, we implemented an RDMA-enabled \paxos 
protocol~\cite{falcon:github} in Linux. This protocol intercepts a general 
program's inbound socket calls (\eg, \recv) and agrees on the received inputs 
in these calls across replicas of this program. To simulate a typical Twitter 
deployment, we ran this protocol with \redis, a popular key-value store that 
Twitter uses, and we ran \redis's own benchmark workload. Evaluation showed 
that, compared to \redis's unreplicated execution, this protocol incurred 
merely X.X\% overhead on response time and Y.Y\% overhead on throughput. This 
initial evaluation result suggests that \xxx could achieve reasonable 
performance overhead on critical applications.

% P9: Feasibility study.

% P10: rest of the paper.
The remaining of this paper is organized as follows. \S\ref{sec:background} 
introduces background on \paxos and RDMA. \S\ref{sec:overview} gives an 
overview of our \xxx system. \S\ref{sec:tail} describes \xxx's design on 
tail-tolerance. \S\ref{sec:eval} presents evaluation results, 
\S\ref{sec:related} discusses related work, and \S\ref{sec:conclusion} 
concludes.   


