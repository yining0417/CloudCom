\section{Introduction} \label{sec:intro}

% P1: virtualization is good
By allowing multiple servers to be consolidated on a small number of physical hosts 
and simplifying provisioning, virtualization is widely used in computing environments 
of various kinds and scales.

% P2: However, previous approaches still fail to efficiently and reliably provide fault-tolerance in virtualized environment
% checkpoint-recovery based replication suffers from two problems
% 1. performance degradation due to the large amount of state that needs to be synchronized between the primary and the backup machines
% 2. significant VM downtimes
% 3. network delay
Nevertheless, server consolidation exacerbates the consequence of unexpected host failures. 
When VMs are consolidated, failure of a single host may bring down mutiple VMs on the host 
and all applications running thereon, resulting in an unacceptable aggregate loss. 
To accommdote virtual machines with high availability, various approaches have been 
proposed to replicate VMs between hosts continuously throughout VM's execution, but 
they are still unable to ensure fault-tolerance in virtualized environments efficiently 
and reliably. One typical technique is checkpoint-recovery based replication of virtual 
machines. It captures the entire execution state of the running VM at relatively 
high frequency so that changes can be reflected to the backup machine nearly instantly. 
However, it suffers from two problems. The main one is the amount of data that needs to be 
copied from the primary to the backup, which can, in some cases, seriously affect the performance. 
Apart from that, VM downtimes incurred by the checkpoint mechanism can be significant.

% P3: derterministic replay is the other approach which can address some of the drawbacks of checkpoint-recovery based replication
% 
% single point of failure
% slow for multi-processor
On the other hand, derterministic replay is an attractive technique which can address the 
aforementioned drawbacks of checkpoint-recovery based replication: primary and backup execute 
the same sequence of instructions, with non-deterministic events injected exactly at the same 
point into both replicas. However, in the existing works, derterministic replay suffers from 
"single point of failure" in order to avoid split brain.
Paxos, a consensus protocol. 
On the other hand, 

First, 
Fortunately, two majority replication schemes (e.g., \paxos) can cpoe this problem. However, to 
leverage \paxos remains challenging. First, 
% To answer Heming's question: old leader and new leader get to an inconsistent state
% After recovery, the old leader should be started in the state as the new leader before participating

% 6.824 2016 mit
% What if the network between primary/backup fails?
%  -Primary is still running
%  -Backup becomes a new primary
%  -Two primaries at the same time!
% one can avoid split brain using a single "master"
% master computer decides whether replica A or replica B is the primary
%   there's just one master, so it never disagrees with itself
% clients talk to the master
% this is probably what VMware FT does (atomic test-and-set in shared disk)
% but what if the master fails?
%   it's a "single point of failure" -- not so good
% VMware FT shared disk atomic test-and-set was (apparently) not replicated

% In the non-shared-disk configuration, there may be no shared storage to use for dealing with a split-brain situation.
% In this case, the system could use some other external tiebreaker, such as a third-party server that both servers can talk to.
% If the servers are part of a cluster, the system could alternatively use a majority algorithm based on cluster membership.

% P3: log-replay is highly architecture-specific and slow for multi-processor

% requiring that the system have a comprehensive understanding of the instruction set being executed and the sources of external events.
% reproducing the exact order in which CPU cores access the shared memory

% P4: Our key insight is to enforce the same ordering over network interaction at all nodes

First, \paxos is notoriously slow because each decision takes at least three message delays between 
when a replica proposes a command and when some replica learns which command has been chosen.

Fortunately, Remote Direct Memory Access (RDMA)-capable networks have dropped in price and made 
substantial inroads into datacenters. It allows one computer to directly access the memory of 
a remote computer without involving the operating system at any host. This enables zero-copy 
transfers, reducing latency and CPU overhead.

This paper presents \xxx, a reliable high-performance SMR system that provides fault-tolerance 
in virtualized environment. Within each replica, \xxx interposes on the network syscalls to keep 
replicas in sync. Specifically, it considers each incoming network system call  an input request, 
and runs a RDMA-based \paxos consensus protocol to ensure that a quorum of the replicas sees the 
same exact sequence of the incoming network syscalls. To tackle nondeterminism, \xxx intercepts 
the outgoing syscalls in the hypervisor and invokes an efficient network output checking protocol 
to detect divergence of execution states.

The remainder of the paper is organized as follows.
