\section{Introduction} \label{sec:intro}

% P1: lots of applications require various big data frameworks. Trading, fraud 
% detection, aviation, medical, military. But existing systems can not meet the 
% high latency and availability requriements of these applications.
Driven the the drastically increasing computational demands and the volumes of 
data, cluster 
management systems~\cite{borg:eurosys15,mesos:nsdi11,tupperware,yarn:socc13,
autopilot:sosp07,quincy:sosp09,apollo:osdi14,fuxi:vldb14} 
incorporate more and more applications. Applications not only include typical 
applications (\eg, Hadoop~\cite{hadoop}), but also \emph{critical} 
applications such as trading, fraud detection, health care, and military 
applications. These applications naturally require high-availability and low 
performance overhead in deployments. For example, a high-frequency trading 
application tends to be highly-available during its operation hours, and adding 
merely hundreds of \us to its response time means big money 
lost~\cite{nosql:finance}.
% This 
% paper considers an application \emph{critical} if it has high requirements on 
% availability and response time. 

% There are already enormous advances in building and applying cluster 
% management systems~\cite{borg:eurosys15,mesos:nsdi11,tupperware,yarn:socc13,
% autopilot:sosp07,quincy:sosp09,apollo:osdi14,fuxi:vldb14}. 

Unfortunately, despite much effort, these systems are still difficult to meet 
the high requirements of critical applications because these systems do not 
provide high-availability to the applications. Specifically, existing systems 
typically replicate their job controller component which accepts tasks, allocate 
computational resources, and (re)schedule tasks on available resources. However, 
the applications themselves are not replicated: if an application crashes or a 
computational resource goes down (\eg, hardware errors), these systems have to 
reschedule the tasks, leaving an arbitrary unavailable time window for these 
applications.
% A possible key reason of this problem is that these systems are 
% initially designed for big-data engines which already considered fault recovery 
% (but, not availability).

% P2: SMR a promising approach, but slow.
State machine replication (SMR)~\cite{paxos} is a promising approach 
to address the availability problem of critical applications. SMR runs the 
same program on a number of replicas and uses a distributed consensus protocol 
(\eg, 
\paxos~\cite{paxos:practical,paxos,paxos:simple,paxos:complex,epaxos:sosp13}) 
to enforce the same inputs among 
replicas. An input consensus can achieve as long as a majority of replicas 
agree, thus SMR can tolerate various faults such as minor replica failures. For 
instance, two existing systems \borg~\cite{borg:eurosys15} and 
\mesos~\cite{mesos:nsdi11} use \paxos to replicate their controllers.

% Typically, \paxos assigns a replica as the leader to propose 
% consensus requests, and the other replicas agree or reject requests. 


However, \paxos's consensus is notoriously difficult to be fast. 
To agree on an input, traditional consensus protocols invoke at least one 
message round-trip between two replicas. Given that a \v{ping} in Ethernet 
takes hundreds of \us, a program running in an SMR system with three replicas 
must wait at least this time before processing an input. 

% P3: RDMA, new opportunity. Two RDMA systems.
Recently, Remote Direct Access Memory (RDMA) becomes commonplace in datacenters 
because of the decreasing prices of RDMA hardware. RDMA allows a process to 
directly write to the user space memory of a remote process, completely 
bypassing the remote OS kernel or CPU (the so called ``one-sided" operations). 
An evaluation~\cite{pilaf:usenix14} shows that such a write round-trip takes 
only $\sim$3 \us in the Infiniband architecture~\cite{infiniband}. Two recent 
RDMA-enabled \paxos implementations~\cite{dare:hpdc15,falcon:github} reported 
$\sim$15 \us in consensus latency.

A naive approach to achieve high-availability for critical applications could 
be integrating \paxos within every application. However, doing so has two 
issues. First, \paxos is notoriously difficult 
to understand~\cite{raft:usenix14}, implement~\cite{paxos:practical}, and 
verify~\cite{demeter:sosp11}.

Second, running a \paxos-enabled application in a 
cluster management system could be problematic because the system is unaware 
of the replication logic. For instance, if the system happens to schedule all 
the replicas of the same job on the same machine, then a single-point failure 
of this machine will turn down all replicas. 
% Integrating \paxos to an application requires 
% strong \paxos expertise for the application developers

% As a common 
% RDMA practice, to ensure that such a write successfully resides in the 
% memory of a remote process, the local process should wait until the remote NIC 
% (network interface card) sends an ACK to the local host's NIC. 

% P4: We present a new fast, highly-available computing platform by building a 
% eco system with Mesos and RDMA SMR systems.
This paper proposes the design of \xxx, a cluster management system that 
automatically provides high-availability to general applications. \xxx makes 
replicas of controllers agree on job requests efficiently with 
RDMA-enabled \paxos protocols~\cite{dare:hpdc15, falcon:github}. To avoid the 
two aforementioned issues, \xxx chooses to integrate \paxos in the 
cluster system, not in applications. Doing so has two benefits. First, \xxx's 
\paxos acts a single, general SMR service to applications: we can just 
leverage existing verification tools~\cite{modist:nsdi09,demeter:sosp11} to 
make sure this \paxos protocol is robust and correct, and then we can benefit 
many applications. Second, now \xxx's own scheduler can handle the 
replication logic and do careful, replication-aware scheduling for jobs.

%  In \xxx, a 
% replicas of controllers run a critical application's same task on diverse 
% physical machines, and controllers use a fast \paxos protocol to agree on 
% computational 
% requests. 

% Benefits.
In an implementation level, \xxx integrates a RDMA-enabled \paxos 
protocol~\cite{falcon:github} with \mesos~\cite{mesos:nsdi11}, a widely used 
cluster management system. This integration forms a mutual beneficial 
eco-system\footnote{We name 
our system after the ancient Chinese three-legged tripod, a reliable, 
multi-purpose container.}: by replicating tasks with \paxos, \xxx ensures that 
minor failures in a task does not affect the other tasks to compute results 
efficiently. By leveraging \mesos's resource allocation and isolation feature, 
\xxx can run diverse applications on the same physical machine, saving much of 
the physical hardware cost in traditional replication techniques.

% P5: Applications enjoy better latency and fault-tolerance.

% P6: Benefit, now frameworks can focus on their own logic, no longer need 
% fault-toelrance module. Fault-tolerance requires expert knowledge, really 
% hard to build.

% P7: Can also address the performance problem: slow tail.
% A happy side-effect in \xxx is that it 
% addresses a pervasive latency-tail challenge via its fault-tolerance 
% architecture. 

In addition to fault-tolerance, \xxx also provides a tail-tolerance 
feature~\cite{tail:cacm13} in cluster computing. The tail of latency 
distribution problem (or straggler) is pervasive for decades: since a 
job request consists of tasks on diverse machines, the response of this 
job is often bounded by the slowest task. Although \xxx is not the first work 
that presents a replication approach to address the straggler 
problem~\cite{dolly:nsdi13}, \xxx provides this feature in a simple and general 
way on top of its \paxos replication architecture.

% We propose a general approach with 
% \xxx's fault-tolerance architecture: \xxx collects replicas' output on a request 
% with its consensus protocol, it then replies to client programs with the 
% fastest replica's result. Therefore, applications can safely ignore this problem 
% and focus on their application logic.

% P8: in essense, a datacenter OS: handles resource allocation/isolation, 
% fault-tolerance, and performance for frameworks (applications). Use 
% replication to bstract away % physical machine details; frameworks now only 
% see computing resources, safely ignore other things such as physical 
% machines, fault-tolerance, and tails.
As part of \xxx's development, we have implemented an RDMA-enabled \paxos 
protocol~\cite{falcon:github} in Linux. This protocol intercepts a general 
program's inbound socket calls (\eg, \recv) and agrees on the received inputs 
in these calls across replicas of the same program. To emulate a typical 
Twitter deployment, we ran this protocol with \redis, a popular key-value store 
that Twitter uses, and we ran \redis's own benchmark workload. Evaluation 
showed that, compared to \redis's unreplicated execution, this protocol 
incurred merely \latencyoverhead\% overhead on response time and 
\tputoverhead\% overhead on throughput. This protocol was 40.1X faster 
than \zookeeper's SMR protocol~\cite{calvin:sigmod12} which runs on TCP/IP.
% These initial evaluation results suggest that 
% \xxx can achieve reasonable performance overhead on critical applications.

% P9: Feasibility study.

% P10: rest of the paper.
The remaining of this paper is organized as follows. \S\ref{sec:background} 
introduces background on \paxos and RDMA. \S\ref{sec:overview} gives an 
overview of our \xxx design. \S\ref{sec:tail} describes \xxx's design on 
tail-tolerance. \S\ref{sec:eval} presents evaluation results, 
\S\ref{sec:related} discusses related work, and \S\ref{sec:conclusion} 
concludes.   


